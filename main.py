import os
import re
import time
from datetime import datetime
import pandas as pd
from pathlib import Path

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# --- å®šæ•° ---
KEYWORD = "æ—¥ç”£"
EXCEL_FILENAME = "nissan_yahoo_news.xlsx"

def format_datetime(dt_obj):
    """datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æŒ‡å®šã®æ›¸å¼ï¼ˆYYYY/MM/DD HH:MMï¼‰ã®æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹"""
    return dt_obj.strftime("%Y/%m/%d %H:%M")

def get_yahoo_news_with_selenium(keyword: str) -> list[dict]:
    """
    Seleniumã‚’ä½¿ã£ã¦Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹
    """
    print("--- Yahoo! News ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ ---")
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
    driver.get(search_url)
    time.sleep(5) # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ

    # è¤‡æ•°å›ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦è¨˜äº‹ã‚’æœ€å¤§é™èª­ã¿è¾¼ã‚€
    for _ in range(5):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    articles = soup.find_all("li", class_=re.compile(r"SearchList_item"))
    articles_data = []

    for article in articles:
        try:
            title_tag = article.find("p", class_=re.compile(r"SearchList_title"))
            title = title_tag.text.strip() if title_tag else ""

            link_tag = article.find("a", href=True)
            url = link_tag["href"] if link_tag else ""

            time_tag = article.find("time")
            date_str = time_tag.text.strip() if time_tag else ""
            formatted_date = "å–å¾—ä¸å¯"
            if date_str:
                date_str_no_day = re.sub(r'\s*\ï¼ˆ[æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\ï¼‰', '', date_str)
                try:
                    dt_obj = datetime.strptime(f"{datetime.now().year}/{date_str_no_day}", "%Y/%m/%d %H:%M")
                    formatted_date = format_datetime(dt_obj)
                except ValueError:
                    formatted_date = date_str

            source_tag = article.find("p", class_=re.compile(r"SearchList_provider"))
            source_text = source_tag.text.strip() if source_tag else "å–å¾—ä¸å¯"

            if title and url:
                articles_data.append({
                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                    "URL": url,
                    "æŠ•ç¨¿æ—¥": formatted_date,
                    "å¼•ç”¨å…ƒ": source_text
                })
        except Exception as e:
            print(f"âš ï¸ è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä»¶æ•°: {len(articles_data)} ä»¶")
    return articles_data

def write_to_excel(articles: list[dict], filename: str):
    """
    è¨˜äº‹ãƒªã‚¹ãƒˆã‚’Excelãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€ã€‚
    - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°æ–°ã—ã„è¨˜äº‹ã®ã¿ã‚’è¿½è¨˜ã™ã‚‹ã€‚
    - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªãè¨˜äº‹ã‚‚ãªã„å ´åˆã€ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹ã€‚
    """
    new_df = pd.DataFrame(articles)
    excel_file = Path(filename)
    
    # æ—¢å­˜ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆ
    if excel_file.exists():
        print(f"ğŸ“– æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
        existing_df = pd.read_excel(excel_file)
        
        # æ–°ã—ã„è¨˜äº‹ãŒãªã„å ´åˆã¯ã“ã“ã§å‡¦ç†ã‚’çµ‚äº†
        if new_df.empty:
            print("âœ… æ–°ã—ã„è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ›´æ–°ã•ã‚Œã¾ã›ã‚“ã€‚")
            return
            
        existing_urls = set(existing_df['URL'])
        new_articles_df = new_df[~new_df['URL'].isin(existing_urls)]

        if new_articles_df.empty:
            print("âœ… æ–°ã—ã„è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ›´æ–°ã•ã‚Œã¾ã›ã‚“ã€‚")
            return
            
        print(f"â• {len(new_articles_df)}ä»¶ã®æ–°ã—ã„è¨˜äº‹ã‚’è¿½è¨˜ã—ã¾ã™ã€‚")
        combined_df = pd.concat([existing_df, new_articles_df], ignore_index=True)

    # æ—¢å­˜ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆ
    else:
        # æ–°ã—ãå–å¾—ã—ãŸè¨˜äº‹ã‚‚ãªã„å ´åˆã€ãƒ˜ãƒƒãƒ€ãƒ¼ã ã‘ã®ç©ºã®DataFrameã‚’ä½œæˆ
        if new_df.empty:
            print(f"ğŸ“„ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸãŒã€ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿ã®æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’ä½œæˆã—ã¾ã™ã€‚")
            combined_df = pd.DataFrame(columns=['ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'æŠ•ç¨¿æ—¥', 'å¼•ç”¨å…ƒ'])
        # æ–°ã—ãå–å¾—ã—ãŸè¨˜äº‹ãŒã‚ã‚‹å ´åˆã€ãã‚ŒãŒãã®ã¾ã¾æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã¨ãªã‚‹
        else:
            print(f"ğŸ“„ æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’ä½œæˆã—ã¾ã™ã€‚")
            combined_df = new_df
            
    # ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ã‚½ãƒ¼ãƒˆå‡¦ç†ã‚’è¡Œã†
    if not combined_df.empty and 'æŠ•ç¨¿æ—¥' in combined_df.columns:
        combined_df['æŠ•ç¨¿æ—¥æ™‚'] = pd.to_datetime(combined_df['æŠ•ç¨¿æ—¥'], format='%Y/%m/%d %H:%M', errors='coerce')
        combined_df.sort_values(by='æŠ•ç¨¿æ—¥æ™‚', ascending=False, inplace=True, na_position='last')
        final_df = combined_df.drop(columns=['æŠ•ç¨¿æ—¥æ™‚'])
    else:
        final_df = combined_df

    # Excelãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—
    try:
        final_df.to_excel(filename, index=False, engine='openpyxl')
        print(f"ğŸ’¾ Excelãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã¸ã®æ›¸ãè¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ç·ä»¶æ•°: {len(final_df)} ä»¶")
    except Exception as e:
        print(f"âŒ Excelãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®æ›¸ãè¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    yahoo_news_articles = get_yahoo_news_with_selenium(KEYWORD)
    # å–å¾—ã—ãŸè¨˜äº‹ãƒªã‚¹ãƒˆã‚’å¸¸ã«write_to_excelé–¢æ•°ã«æ¸¡ã™
    write_to_excel(yahoo_news_articles, EXCEL_FILENAME)
