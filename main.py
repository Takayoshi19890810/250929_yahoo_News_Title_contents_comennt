import os
import re
import time
from datetime import datetime, timedelta
import pandas as pd
from pathlib import Path

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# --- å®šæ•° ---
KEYWORD = "æ—¥ç”£"
EXCEL_FILENAME = "nissan_yahoo_news.xlsx"

def format_datetime(dt_obj):
    """datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æŒ‡å®šã®æ›¸å¼ï¼ˆYYYY/MM/DD HH:MMï¼‰ã®æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹"""
    return dt_obj.strftime("%Y/%m/%d %H:%M")

def get_yahoo_news_with_selenium(keyword: str) -> list[dict]:
    """
    Seleniumã‚’ä½¿ã£ã¦Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹
    """
    print("--- Yahoo! News ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ ---")
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
    driver.get(search_url)
    time.sleep(5) # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ

    # è¤‡æ•°å›ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦è¨˜äº‹ã‚’æœ€å¤§é™èª­ã¿è¾¼ã‚€
    for _ in range(5):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    articles = soup.find_all("li", class_=re.compile(r"SearchList_item"))
    articles_data = []

    for article in articles:
        try:
            title_tag = article.find("p", class_=re.compile(r"SearchList_title"))
            title = title_tag.text.strip() if title_tag else ""

            link_tag = article.find("a", href=True)
            url = link_tag["href"] if link_tag else ""

            time_tag = article.find("time")
            date_str = time_tag.text.strip() if time_tag else ""
            formatted_date = "å–å¾—ä¸å¯"
            if date_str:
                # "ï¼ˆæœˆï¼‰"ãªã©ã®æ›œæ—¥è¡¨è¨˜ã‚’å‰Šé™¤
                date_str_no_day = re.sub(r'\s*\ï¼ˆ[æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\ï¼‰', '', date_str)
                try:
                    # 'M/D HH:mm'å½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹
                    dt_obj = datetime.strptime(f"{datetime.now().year}/{date_str_no_day}", "%Y/%m/%d %H:%M")
                    formatted_date = format_datetime(dt_obj)
                except ValueError:
                    # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯å…ƒã®æ–‡å­—åˆ—ã‚’ãã®ã¾ã¾åˆ©ç”¨
                    formatted_date = date_str

            source_tag = article.find("p", class_=re.compile(r"SearchList_provider"))
            source_text = source_tag.text.strip() if source_tag else "å–å¾—ä¸å¯"

            if title and url:
                articles_data.append({
                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                    "URL": url,
                    "æŠ•ç¨¿æ—¥": formatted_date,
                    "å¼•ç”¨å…ƒ": source_text
                })
        except Exception as e:
            print(f"âš ï¸ è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä»¶æ•°: {len(articles_data)} ä»¶")
    return articles_data

def write_to_excel(articles: list[dict], filename: str):
    """
    è¨˜äº‹ãƒªã‚¹ãƒˆã‚’Excelãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€ã€‚æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°æ–°ã—ã„è¨˜äº‹ã®ã¿ã‚’è¿½è¨˜ã™ã‚‹ã€‚
    """
    if not articles:
        print("âš ï¸ è¿½è¨˜ã™ã‚‹æ–°ã—ã„è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # æ–°ã—ãå–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
    new_df = pd.DataFrame(articles)
    
    # æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ï¼ˆå¤‰æ›ã§ããªã„ã‚‚ã®ã¯NaTï¼‰
    new_df['æŠ•ç¨¿æ—¥æ™‚'] = pd.to_datetime(new_df['æŠ•ç¨¿æ—¥'], format='%Y/%m/%d %H:%M', errors='coerce')

    excel_file = Path(filename)
    if excel_file.exists():
        print(f"ğŸ“– æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
        try:
            existing_df = pd.read_excel(excel_file)
            existing_urls = set(existing_df['URL'])

            # æ—¢å­˜ãƒªã‚¹ãƒˆã«ãªã„URLã®è¨˜äº‹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            new_articles_df = new_df[~new_df['URL'].isin(existing_urls)].copy()
            
            if new_articles_df.empty:
                print("âœ… æ–°ã—ã„è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ›´æ–°ã•ã‚Œã¾ã›ã‚“ã€‚")
                return

            print(f"â• {len(new_articles_df)}ä»¶ã®æ–°ã—ã„è¨˜äº‹ã‚’è¿½è¨˜ã—ã¾ã™ã€‚")
            
            # æ—¢å­˜ã®DataFrameã¨æ–°ã—ã„è¨˜äº‹ã®DataFrameã‚’çµåˆ
            combined_df = pd.concat([existing_df, new_articles_df], ignore_index=True)
            # æ—¥ä»˜åˆ—ã‚‚åŒæ§˜ã«å¤‰æ›
            combined_df['æŠ•ç¨¿æ—¥æ™‚'] = pd.to_datetime(combined_df['æŠ•ç¨¿æ—¥'], format='%Y/%m/%d %H:%M', errors='coerce')

        except Exception as e:
            print(f"âš ï¸ Excelãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ãã—ã¾ã™ã€‚")
            combined_df = new_df

    else:
        print(f"ğŸ“„ æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’ä½œæˆã—ã¾ã™ã€‚")
        combined_df = new_df

    # æŠ•ç¨¿æ—¥æ™‚ã§é™é †ã«ã‚½ãƒ¼ãƒˆï¼ˆNaTã¯æœ«å°¾ã¸ï¼‰
    combined_df.sort_values(by='æŠ•ç¨¿æ—¥æ™‚', ascending=False, inplace=True, na_position='last')
    
    # ä¸€æ™‚çš„ãªæ—¥æ™‚åˆ—ã‚’å‰Šé™¤
    final_df = combined_df.drop(columns=['æŠ•ç¨¿æ—¥æ™‚'])

    # Excelãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—
    try:
        final_df.to_excel(filename, index=False, engine='openpyxl')
        print(f"ğŸ’¾ Excelãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã¸ã®æ›¸ãè¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ç·ä»¶æ•°: {len(final_df)} ä»¶")
    except Exception as e:
        print(f"âŒ Excelãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®æ›¸ãè¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


if __name__ == "__main__":
    yahoo_news_articles = get_yahoo_news_with_selenium(KEYWORD)
    if yahoo_news_articles:
        write_to_excel(yahoo_news_articles, EXCEL_FILENAME)
