import os
import json
import time
import re
import pandas as pd
from datetime import datetime
import google.generativeai as genai

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import requests

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# --- è¨­å®šé …ç›® ---
KEYWORD = "æ—¥ç”£"
EXCEL_FILE = "yahoo_news_analysis.xlsx"
AI_MODEL_NAME = "gemini-1.0-pro" # å®‰å®šç‰ˆã®ãƒ¢ãƒ‡ãƒ«å

MAX_BODY_PAGES = 10
MAX_COMMENT_PAGES = 10
MAX_TOTAL_COMMENTS = 500


# --- AIåˆ†æã®è¨­å®š ---
try:
    GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
    if not GEMINI_API_KEY:
        print("è­¦å‘Š: ç’°å¢ƒå¤‰æ•° 'GEMINI_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚AIåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
        AI_ENABLED = False
    else:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel(AI_MODEL_NAME)
        AI_ENABLED = True
except Exception as e:
    print(f"AIãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    AI_ENABLED = False

# --- å…±é€šé–¢æ•° ---

def format_datetime_str(dt_obj):
    return dt_obj.strftime("%Y/%m/%d %H:%M")

def get_yahoo_news_with_selenium(keyword: str) -> list[dict]:
    print("--- Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ ---")
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    
    articles_data = []
    try:
        search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
        driver.get(search_url)
        time.sleep(3)

        soup = BeautifulSoup(driver.page_source, "html.parser")
        articles = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))

        for article in articles:
            title_tag = article.find("div", class_=re.compile("sc-3ls169-0"))
            link_tag = article.find("a", href=True)
            time_tag = article.find("time")
            source_tag_outer = article.find("div", class_=re.compile("sc-n3vj8g-0"))
            
            if not all([title_tag, link_tag, time_tag, source_tag_outer]):
                continue

            title = title_tag.text.strip()
            url = link_tag["href"]
            date_str = time_tag.text.strip()
            source = source_tag_outer.text.strip()
            
            try:
                cleaned_date_str = re.sub(r'\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)', '', date_str)
                dt_obj = datetime.strptime(cleaned_date_str, "%Y/%m/%d %H:%M")
                formatted_date = format_datetime_str(dt_obj)
            except ValueError:
                formatted_date = date_str

            if title and url:
                articles_data.append({
                    "ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": url, "æŠ•ç¨¿æ—¥": formatted_date, "å¼•ç”¨å…ƒ": source
                })
    finally:
        driver.quit()
        
    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰ {len(articles_data)} ä»¶ã®è¨˜äº‹æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
    return articles_data

def fetch_article_pages(base_url: str) -> list[str]:
    body_pages = []
    for page in range(1, MAX_BODY_PAGES + 1):
        url = base_url if page == 1 else f"{base_url}?page={page}"
        try:
            res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")
            
            article_body = soup.find("div", class_=re.compile("article_body"))
            if not article_body:
                if page > 1: break
                continue

            page_text = "\n".join(p.get_text(strip=True) for p in article_body.find_all("p"))
            
            if not page_text or (body_pages and page_text == body_pages[-1]):
                break
            
            body_pages.append(page_text)
            time.sleep(1)
        except requests.RequestException:
            break
    return body_pages

def fetch_comments_with_selenium(base_url: str) -> tuple[int, list[str]]:
    total_comments = []
    per_page_comments_text = []

    if "/articles/" not in base_url:
        return 0, []

    article_id = base_url.split("/articles/")[-1]
    comment_base_url = f"https://news.yahoo.co.jp/articles/{article_id}/comments"
    
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    
    try:
        for page in range(1, MAX_COMMENT_PAGES + 1):
            comment_page_url = f"{comment_base_url}?page={page}"
            driver.get(comment_page_url)

            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "ul[class*='comments-list']"))
                )
            except TimeoutException:
                if page == 1:
                    print("  - ã‚³ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰ã€‚")
                break

            soup = BeautifulSoup(driver.page_source, "html.parser")
            
            selectors = [
                "p.sc-169yn8p-10",
                "p[data-ylk*='cm_body']",
                "p[class*='comment']",
                "div[data-testid='comment-body-text']",
                "div.commentBody, p.commentBody",
                "div[data-ylk*='cm_body']"
            ]
            
            p_candidates = []
            for sel in selectors:
                p_candidates.extend(soup.select(sel))

            current_page_comments = [p.get_text(strip=True) for p in p_candidates if p.get_text(strip=True)]
            current_page_comments = list(dict.fromkeys(current_page_comments))

            if not current_page_comments:
                break

            total_comments.extend(current_page_comments)
            per_page_comments_text.append("\n---\n".join(current_page_comments))

            if len(total_comments) >= MAX_TOTAL_COMMENTS:
                break
    except Exception as e:
        print(f"  - ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        driver.quit()
        
    return len(total_comments), per_page_comments_text

def analyze_article_with_ai(title: str, body_text: str) -> dict:
    default_response = {
        "title_sentiment": "N/A", "title_category": "N/A",
        "body_sentiment": "N/A", "body_category": "N/A"
    }

    if not AI_ENABLED or not title:
        return default_response

    analyze_body = bool(body_text and isinstance(body_text, str) and len(body_text.strip()) >= 10)

    optional_keys_string = ""
    if analyze_body:
        optional_keys_string = '\n- "body_sentiment"\n- "body_category"'

    prompt = f"""
    ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€ã¨ã€Œæœ¬æ–‡ã€ã‚’åˆ†æã—ã€çµæœã‚’å˜ä¸€ã®JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚

    åˆ†æé …ç›®:
    1. sentiment: å…¨ä½“ã®è«–èª¿ãŒã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã€ã®ã„ãšã‚Œã‹ã€‚
    2. category: å†…å®¹ã«æœ€ã‚‚åˆè‡´ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’ä»¥ä¸‹ã‹ã‚‰1ã¤é¸æŠã—ã¦ãã ã•ã„ã€‚
       ["æ–°æŠ€è¡“ãƒ»ç ”ç©¶é–‹ç™º", "çµŒå–¶ãƒ»è²¡å‹™", "è²©å£²ãƒ»ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "ç”Ÿç”£ãƒ»å“è³ª", "äººäº‹ãƒ»åŠ´å‹™", "ä¸ç¥¥äº‹ãƒ»è¨´è¨Ÿ", "ãã®ä»–"]

    åˆ†æå¯¾è±¡:
    ---
    ã‚¿ã‚¤ãƒˆãƒ«: {title}
    ---
    æœ¬æ–‡: {body_text[:2000] if analyze_body else "ï¼ˆæœ¬æ–‡ãªã—ï¼‰"}
    ---

    JSONã®ã‚­ãƒ¼ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«æŒ‡å®šã—ã¦ãã ã•ã„:
    - "title_sentiment"
    - "title_category"{optional_keys_string}

    JSONå‡ºåŠ›ã®ã¿è¨˜è¿°ã—ã¦ãã ã•ã„:
    """
    try:
        response = genai.GenerativeModel(AI_MODEL_NAME).generate_content(prompt)
        json_str = response.text.strip().replace("```json", "").replace("```", "")
        analysis_result = json.loads(json_str)

        return {
            "title_sentiment": analysis_result.get("title_sentiment", "N/A"),
            "title_category": analysis_result.get("title_category", "N/A"),
            "body_sentiment": analysis_result.get("body_sentiment", "N/A"),
            "body_category": analysis_result.get("body_category", "N/A"),
        }
    except Exception as e:
        print(f"  - AIåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        return default_response

def main():
    print("--- ãƒ—ãƒ­ã‚°ãƒ©ãƒ é–‹å§‹ ---")
    
    try:
        df_existing = pd.read_excel(EXCEL_FILE)
        existing_urls = set(df_existing['URL'])
        print(f"ğŸ” æ—¢å­˜ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚({len(existing_urls)}ä»¶ã®è¨˜äº‹)")
    except FileNotFoundError:
        df_existing = pd.DataFrame()
        existing_urls = set()
        print("ğŸ” æ—¢å­˜ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æ–°è¦ã«ä½œæˆã—ã¾ã™ã€‚")

    articles = get_yahoo_news_with_selenium(KEYWORD)
    if not articles:
        print("âš ï¸ å‡¦ç†å¯¾è±¡ã®è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    new_articles_data = []
    for i, article in enumerate(articles):
        if article['URL'] in existing_urls:
            continue
        
        print(f"\n--- æ–°è¦è¨˜äº‹ã®å‡¦ç† ({i+1}/{len(articles)}): {article['ã‚¿ã‚¤ãƒˆãƒ«']} ---")
        
        print("  - æœ¬æ–‡ã‚’ãƒšãƒ¼ã‚¸ã”ã¨ã«å–å¾—ä¸­...")
        body_pages = fetch_article_pages(article['URL'])
        print(f"  - æœ¬æ–‡ã‚’ {len(body_pages)} ãƒšãƒ¼ã‚¸åˆ†å–å¾—ã—ã¾ã—ãŸã€‚")
        
        print("  - ã‚³ãƒ¡ãƒ³ãƒˆã‚’ãƒšãƒ¼ã‚¸ã”ã¨ã«å–å¾—ä¸­...")
        comments_count, comment_pages = fetch_comments_with_selenium(article['URL'])
        print(f"  - åˆè¨ˆ {comments_count} ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ {len(comment_pages)} ãƒšãƒ¼ã‚¸åˆ†å–å¾—ã—ã¾ã—ãŸã€‚")

        print("  - AIã«ã‚ˆã‚‹åˆ†æã‚’å®Ÿè¡Œä¸­ (APIæ¶ˆè²»é‡æœ€é©åŒ–ç‰ˆ)...")
        first_page_body = body_pages[0] if body_pages else ""
        analysis = analyze_article_with_ai(article['ã‚¿ã‚¤ãƒˆãƒ«'], first_page_body)
        
        new_article_dict = {
            'ã‚¿ã‚¤ãƒˆãƒ«': article['ã‚¿ã‚¤ãƒˆãƒ«'],
            'URL': article['URL'],
            'æŠ•ç¨¿æ—¥': article['æŠ•ç¨¿æ—¥'],
            'å¼•ç”¨å…ƒ': article['å¼•ç”¨å…ƒ'],
            'ã‚³ãƒ¡ãƒ³ãƒˆæ•°': comments_count,
            'ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ãƒã‚¸ãƒã‚¬åˆ¤å®š': analysis.get('title_sentiment'),
            'ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªåˆ†ã‘': analysis.get('title_category'),
            'ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ãƒã‚¸ãƒã‚¬åˆ¤å®š': analysis.get('body_sentiment'),
            'ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªåˆ†ã‘': analysis.get('body_category'),
        }

        for idx, page_content in enumerate(body_pages):
            new_article_dict[f'æœ¬æ–‡({idx+1}ãƒšãƒ¼ã‚¸ç›®)'] = page_content
        
        for idx, page_content in enumerate(comment_pages):
            new_article_dict[f'é–²è¦§è€…ã‚³ãƒ¡ãƒ³ãƒˆ({idx+1}ãƒšãƒ¼ã‚¸ç›®)'] = page_content
            
        new_articles_data.append(new_article_dict)
        time.sleep(1)

    if new_articles_data:
        df_new = pd.DataFrame(new_articles_data)
        df_combined = pd.concat([df_existing, df_new], ignore_index=True)
        
        static_columns = [
            'ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'æŠ•ç¨¿æ—¥', 'å¼•ç”¨å…ƒ', 'ã‚³ãƒ¡ãƒ³ãƒˆæ•°',
            'ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ãƒã‚¸ãƒã‚¬åˆ¤å®š', 'ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªåˆ†ã‘',
            'ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ãƒã‚¸ãƒã‚¬åˆ¤å®š', 'ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹æœ¬æ–‡ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªåˆ†ã‘'
        ]
        
        body_columns = sorted([col for col in df_combined.columns if col.startswith('æœ¬æ–‡(')])
        comment_columns = sorted([col for col in df_combined.columns if col.startswith('é–²è¦§è€…ã‚³ãƒ¡ãƒ³ãƒˆ(')])
        
        final_column_order = static_columns + body_columns + comment_columns
        
        final_df = df_combined[[col for col in final_column_order if col in df_combined.columns]]
        
        final_df.to_excel(EXCEL_FILE, index=False)
        print(f"\nâœ… {len(new_articles_data)}ä»¶ã®æ–°è¦è¨˜äº‹ã‚’ '{EXCEL_FILE}' ã«è¿½è¨˜ã—ã¾ã—ãŸã€‚")
    else:
        print("\nâš ï¸ è¿½è¨˜ã™ã¹ãæ–°ã—ã„è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    print("--- ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº† ---")

if __name__ == "__main__":
    main()
