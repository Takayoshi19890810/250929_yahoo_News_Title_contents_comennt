import os
import re
import time
from datetime import datetime
import pandas as pd
from pathlib import Path

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# --- å®šæ•° ---
KEYWORD = "æ—¥ç”£"
EXCEL_FILENAME = "yahoo_news_articles.xlsx"


def get_yahoo_news_with_selenium(keyword: str) -> list[dict]:
    """
    Seleniumã‚’ä½¿ã£ã¦Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹
    """
    print("--- Yahoo! News ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ ---")
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
    
    articles_data = []
    try:
        driver.get(search_url)
        time.sleep(5)  # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ

        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        list_container = soup.find('ul', class_=re.compile(r"SearchList_"))
        
        if not list_container:
            print("âŒ è¨˜äº‹ãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚µã‚¤ãƒˆæ§‹é€ ãŒå¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            return []

        articles = list_container.find_all('li', class_=re.compile(r"SearchList_item"))

        for article in articles:
            try:
                title_tag = article.find("p", class_=re.compile(r"SearchList_title"))
                title = title_tag.text.strip() if title_tag else ""

                link_tag = article.find("a", href=True)
                url = link_tag["href"] if link_tag else ""

                time_tag = article.find("time")
                date_str = time_tag.text.strip() if time_tag else "å–å¾—ä¸å¯"
                
                source_tag = article.find("p", class_=re.compile(r"SearchList_provider"))
                source_text = source_tag.text.strip() if source_tag else "å–å¾—ä¸å¯"

                if title and url:
                    articles_data.append({
                        "ã‚¿ã‚¤ãƒˆãƒ«": title,
                        "URL": url,
                        "æŠ•ç¨¿æ—¥": date_str,
                        "å¼•ç”¨å…ƒ": source_text
                    })
            except Exception:
                continue

    except Exception as e:
        print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    finally:
        driver.quit()

    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä»¶æ•°: {len(articles_data)} ä»¶")
    return articles_data


def write_to_excel(articles: list[dict], filename: str):
    """
    è¨˜äº‹ãƒªã‚¹ãƒˆã‚’Excelãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€ã€‚
    - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°æ–°ã—ã„è¨˜äº‹ã®ã¿ã‚’è¿½è¨˜ã™ã‚‹ã€‚
    - æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªãè¨˜äº‹ã‚‚ãªã„å ´åˆã€ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹ã€‚
    """
    new_df = pd.DataFrame(articles)
    file_path = Path(filename)

    if file_path.exists():
        print(f"ğŸ“– æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’èª­ã¿è¾¼ã¿ã¾ã™...")
        existing_df = pd.read_excel(file_path)
        
        if new_df.empty:
            print("âœ… æ–°ã—ã„è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ›´æ–°ã•ã‚Œã¾ã›ã‚“ã€‚")
            return
            
        existing_urls = set(existing_df['URL'])
        new_articles_df = new_df[~new_df['URL'].isin(existing_urls)]
        
        if new_articles_df.empty:
            print("âœ… æ–°ã—ã„è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ›´æ–°ã•ã‚Œã¾ã›ã‚“ã€‚")
            return
            
        print(f"â• {len(new_articles_df)}ä»¶ã®æ–°ã—ã„è¨˜äº‹ã‚’è¿½è¨˜ã—ã¾ã™ã€‚")
        combined_df = pd.concat([existing_df, new_articles_df], ignore_index=True)
    else:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã›ãšã€æ–°ã—ã„è¨˜äº‹ã‚‚ãªã„å ´åˆ
        if new_df.empty:
            print(f"ğŸ“„ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸãŒã€ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿ã®æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’ä½œæˆã—ã¾ã™ã€‚")
            combined_df = pd.DataFrame(columns=['ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'æŠ•ç¨¿æ—¥', 'å¼•ç”¨å…ƒ'])
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã›ãšã€æ–°ã—ã„è¨˜äº‹ãŒã‚ã‚‹å ´åˆ
        else:
            print(f"ğŸ“„ æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’ä½œæˆã—ã¾ã™ã€‚")
            combined_df = new_df

    combined_df = combined_df.drop_duplicates(subset=['URL'], keep='last')
    combined_df = combined_df.sort_values(by='æŠ•ç¨¿æ—¥', ascending=False)

    combined_df.to_excel(filename, index=False, engine='openpyxl')
    print(f"ğŸ’¾ Excelãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã¸ã®æ›¸ãè¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ç·ä»¶æ•°: {len(combined_df)} ä»¶")


if __name__ == "__main__":
    yahoo_news_articles = get_yahoo_news_with_selenium(KEYWORD)
    # å–å¾—ä»¶æ•°ã«é–¢ã‚ã‚‰ãšã€å¸¸ã«æ›¸ãè¾¼ã¿é–¢æ•°ã‚’å‘¼ã³å‡ºã™
    write_to_excel(yahoo_news_articles, EXCEL_FILENAME)
