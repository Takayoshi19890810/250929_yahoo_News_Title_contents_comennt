import os
import re
import time
from datetime import datetime
import pandas as pd
from pathlib import Path

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# --- å®šæ•° ---
KEYWORD = "æ—¥ç”£"
EXCEL_FILENAME = "yahoo_news_articles.xlsx"


def get_yahoo_news_with_selenium(keyword: str) -> list[dict]:
    """
    Seleniumã‚’ä½¿ã£ã¦Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹
    """
    print("--- Yahoo! News ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ ---")
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
    
    articles_data = []
    try:
        driver.get(search_url)
        time.sleep(5)  # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ

        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # è¨˜äº‹ãŒãƒªã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹è¦ªè¦ç´ ã‚’å–å¾—
        list_container = soup.find('ul', class_=re.compile(r"SearchList_"))
        
        if not list_container:
            print("âŒ è¨˜äº‹ãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚µã‚¤ãƒˆæ§‹é€ ãŒå¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            return []

        articles = list_container.find_all('li', class_=re.compile(r"SearchList_item"))

        for article in articles:
            try:
                title_tag = article.find("p", class_=re.compile(r"SearchList_title"))
                title = title_tag.text.strip() if title_tag else ""

                link_tag = article.find("a", href=True)
                url = link_tag["href"] if link_tag else ""

                time_tag = article.find("time")
                date_str = time_tag.text.strip() if time_tag else "å–å¾—ä¸å¯"
                
                source_tag = article.find("p", class_=re.compile(r"SearchList_provider"))
                source_text = source_tag.text.strip() if source_tag else "å–å¾—ä¸å¯"

                if title and url:
                    articles_data.append({
                        "ã‚¿ã‚¤ãƒˆãƒ«": title,
                        "URL": url,
                        "æŠ•ç¨¿æ—¥": date_str,
                        "å¼•ç”¨å…ƒ": source_text
                    })
            except Exception:
                continue # å€‹åˆ¥ã®è¨˜äº‹ã§ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¦ã‚‚å‡¦ç†ã‚’ç¶šã‘ã‚‹

    except Exception as e:
        print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    finally:
        driver.quit()

    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä»¶æ•°: {len(articles_data)} ä»¶")
    return articles_data


def write_to_excel(articles: list[dict], filename: str):
    """
    è¨˜äº‹ãƒªã‚¹ãƒˆã‚’Excelãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€ã€‚æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°æ–°ã—ã„è¨˜äº‹ã®ã¿ã‚’è¿½è¨˜ã™ã‚‹ã€‚
    """
    if not articles:
        print("âš ï¸ æ–°ã—ã„è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    new_df = pd.DataFrame(articles)
    file_path = Path(filename)

    if file_path.exists():
        print(f"ğŸ“– æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’èª­ã¿è¾¼ã¿ã¾ã™...")
        existing_df = pd.read_excel(file_path)
        existing_urls = set(existing_df['URL'])
        
        # æ—¢å­˜ãƒªã‚¹ãƒˆã«ãªã„URLã®è¨˜äº‹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        new_articles_df = new_df[~new_df['URL'].isin(existing_urls)]
        
        if new_articles_df.empty:
            print("âœ… æ–°ã—ã„è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ›´æ–°ã•ã‚Œã¾ã›ã‚“ã€‚")
            return
            
        print(f"â• {len(new_articles_df)}ä»¶ã®æ–°ã—ã„è¨˜äº‹ã‚’è¿½è¨˜ã—ã¾ã™ã€‚")
        combined_df = pd.concat([existing_df, new_articles_df], ignore_index=True)
    else:
        print(f"ğŸ“„ æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’ä½œæˆã—ã¾ã™ã€‚")
        combined_df = new_df

    # é‡è¤‡ã‚’æœ€çµ‚ç¢ºèªã—ã¦å‰Šé™¤ã—ã€æŠ•ç¨¿æ—¥ã§ã‚½ãƒ¼ãƒˆï¼ˆæ—¥ä»˜ã‚‰ã—ãæ–‡å­—åˆ—ã§ã‚½ãƒ¼ãƒˆã‚’è©¦ã¿ã‚‹ï¼‰
    combined_df = combined_df.drop_duplicates(subset=['URL'], keep='last')
    combined_df = combined_df.sort_values(by='æŠ•ç¨¿æ—¥', ascending=False)

    combined_df.to_excel(filename, index=False, engine='openpyxl')
    print(f"ğŸ’¾ Excelãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã¸ã®æ›¸ãè¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ç·ä»¶æ•°: {len(combined_df)} ä»¶")


if __name__ == "__main__":
    yahoo_news_articles = get_yahoo_news_with_selenium(KEYWORD)
    # è¨˜äº‹ãŒ1ä»¶ä»¥ä¸Šã‚ã‚Œã°æ›¸ãè¾¼ã¿å‡¦ç†ã‚’å®Ÿè¡Œ
    if yahoo_news_articles:
        write_to_excel(yahoo_news_articles, EXCEL_FILENAME)
